import torchimport config as cfgimport torch.nn.functional as Fimport numpy as npimport randomimport matplotlib.pyplot as pltimport seaborn as snsfrom utils.torch_extension import binary_concrete, edge_accuracy, sym, sym_transpose, symmetrize, sym_transposev2, \    gumbel_softmax, sample_gumbel_maxfrom utils.metric import entropy, js_divergence, kl_divergence, nll_gaussian, min_ADE, min_FDEfrom instructors.instructors import Instructorfrom construct import normalize, denormalizefrom itertools import chain, combinations, permutationsfrom torch.utils.data.dataset import TensorDatasetfrom torch import optimfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateaufrom torch.autograd import Variableclass training(Instructor):    def __init__(self, model, data, cmd):        super(training, self).__init__(cmd)        self.model = model        self.data = {key: [torch.tensor(i) for i in value] for key, value in data.items()}        # self.es = torch.LongTensor(es)        # self.size = cmd.agent_size        self.opt = optim.Adam(self.model.parameters(), lr=cfg.lr)        self.scheduler = StepLR(self.opt, step_size=cfg.lr_decay, gamma=cfg.gamma)        self.epoch = cfg.dynamic_training_epoch + cfg.static_training_epoch        self.burn_in_steps = cfg.time_steps - cfg.predict_steps        self.dec_repeat_num = cfg.dec_repeat_num    def train(self):        prefix = '/'.join(cfg.log.split('/')[:-1])        name = '{}/best_model.pth'.format(prefix)        self.log.info('model : encoder {} decoder {}'.format('full', 'full'))        self.log.info('data : num {:05d} train ratio {:.2f} val ratio {:.2f} test ratio {:.2f} segment {:03d}'.format(cfg.data_num, cfg.train, cfg.val, cfg.test, cfg.segment))        self.log.info('train method : lr {:.5f} dif-pattern epoch {:03d} {:03d}'.format(cfg.lr, cfg.static_training_epoch, cfg.dynamic_training_epoch))        self.log.info(            'hyper parameter : edge type {:02d} re-enc gap {:02d} dec modal {:03d} trajectories num {:02d}'.format(cfg.edge_types, cfg.re_encoding_gap, cfg.dec_modal,                                                                              cfg.dec_repeat_num))        best_loss = 1e8        minADE, minFDE = 0, 0        for epoch in range(1, 1 + self.epoch):            if epoch <= cfg.static_training_epoch:                pattern = 'static'            else:                pattern = 'dynamic'            '''            self.model.train()            data = self.load_data2(self.data['train'], cfg.batch_size)            data_states = data[0]            data_category = data[1]            data_states = normalize(data_states)            loss_a = 0.            N = 0.            for states, category in zip(data_states, data_category):                if cfg.using_gpu:                    states = states.cuda()                scale = len(states) / cfg.batch_size                N += scale                # context_hidden = self.model.context_embedding(context_input)                context_hidden = Variable(torch.zeros(cfg.batch_size, cfg.context_emb_hid))                train_loss = self.train_model(states, category, context_hidden, pattern)  # category? context_hidden?                loss_a += scale * train_loss            loss_a /= N            '''            self.log.info('epoch {:03d} pattern {}'.format(epoch, pattern))            train_loss, trainADE, trainFDE = self.train_model(pattern)            # acc = self.report('val', [1])            val_ADE, val_FDE = self.eval(pattern)            # val_cur = max(acc, 1 - acc)            if minADE == 0:                minADE = val_ADE            if val_ADE <= minADE:                minADE = val_ADE                torch.save(self.model.module.state_dict(), name)            # if val_cur > val_best:            # val_best = val_cur            # torch.save(self.model.module.state_dict(), name)            self.scheduler.step()        self.model.module.load_state_dict(torch.load(name))        test_minADE, test_minFDE = self.test('dynamic')    def train_model(self, pattern):        self.model.train()        data = self.load_data(TensorDataset(*self.data['train']), cfg.batch_size)  # [batch_size, steps, node, dim]        loss_all = 0.        N = 0.        minADE, minFDE = [], []        for batch in data:            states, category = batch            states = normalize(states)            if cfg.using_gpu:                states = states.cuda()                category = category.cuda()            scale = len(states) / cfg.batch_size            N += scale            # context_hidden = self.model.context_embedding(context_input)            batch_size = states.size(0)            context_hidden = Variable(torch.zeros(batch_size, cfg.context_emb_hid))            steps = states.shape[1]            groups = steps // cfg.re_encoding_gap            output_burn = None            burn_in_index = self.burn_in_steps // cfg.re_encoding_gap            loss_mse, loss_kl = 0, 0            graph_hidden1, graph_hidden2 = None, None            gru_hidden1, gru_hidden2 = None, None            group_index = 0            start_states = torch.tensor(states[:, 0, :, :].unsqueeze(1).contiguous(), dtype=torch.float32).cuda()            while group_index < burn_in_index:                inputs = states[:, group_index * cfg.re_encoding_gap: (group_index + 1) * cfg.re_encoding_gap, :, :]                burn_in_flag = True                '''                edges, prob, graph_hidden1, graph_hidden2 = self.model.module.infer_relations(inputs, category,                                                                                                      context_hidden,                                                                                                      graph_hidden1,                                                                                                      graph_hidden2,                                                                                                      pattern)  # encoding                # edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample                # predict = []                output, gru_hidden1, gru_hidden2 = self.model.module.predict_states(inputs, edges, context_hidden,                                                                                            group_index,                                                                                            gru_hidden1, gru_hidden2,                                                                                            burn_in_flag)                '''                edges, prob, graph_hidden1, graph_hidden2, output, gru_hidden1, gru_hidden2 = self.model(inputs,                                                                                                         category,                                                                                                         context_hidden,                                                                                                         graph_hidden1,                                                                                                         graph_hidden2,                                                                                                         pattern,                                                                                                         group_index,                                                                                                         gru_hidden1,                                                                                                         gru_hidden2,                                                                                                         burn_in_flag)                if output_burn is None:                    output_burn = torch.cat([start_states, output], dim=1)                else:                    output_burn = torch.cat([output_burn, output], dim=1)  # [batch_size, step, node, dim]                # loss_nll = nll_gaussian(output, target, cfg.scale)                # loss_mse.append(loss_nll)                # prob = logits.softmax(-1)                loss_kl = entropy(prob, prob) / (prob.shape[1] * states.shape[2])                # cur_loss = loss_nll + loss_kl                # loss_train.append(cur_loss)                group_index = group_index + 1                """                target = states[:, 1:burn_in_index*cfg.re_encoding_gap, :, :]                loss_nll = nll_gaussian(output_burn[:, 1:, :, :], target, cfg.scale)                loss = loss_nll + loss_kl                self.optimize(self.opt, loss * cfg.scale)                """            if group_index < groups:                target_pred = states[:, burn_in_index * cfg.re_encoding_gap:, :, :]                min_loss = 0                output_pred = None                for d in range(self.dec_repeat_num):                    temp_gruh1, temp_gruh2 = gru_hidden1, gru_hidden2                    temp_graph_hid1, temp_graph_hid2 = graph_hidden1, graph_hidden2                    predict = None                    m = group_index                    while m < groups:                        if predict is None:                            inputs = output_burn[:, -cfg.re_encoding_gap:, :, :]                        else:                            inputs = predict[:, -cfg.re_encoding_gap:, :, :]                        burn_in_flag = False                        '''                        edges, prob, temp_graph_hid1, temp_graph_hid2 = self.model.module.infer_relations(inputs, category,                                                                                                          context_hidden,                                                                                                          temp_graph_hid1, temp_graph_hid2,                                                                                                          pattern)  # encoding                        # edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample                        output, temp_gruh1, temp_gruh2 = self.model.module.predict_states(inputs, edges, context_hidden, m,                                                                                              temp_gruh1, temp_gruh2,                                                                                              burn_in_flag)                        '''                        edges, prob, temp_graph_hid1, temp_graph_hid2, output, temp_gruh1, temp_gruh2 = self.model(inputs, category, context_hidden, temp_graph_hid1, temp_graph_hid2, pattern, group_index, temp_gruh1, temp_gruh2, burn_in_flag)                        if predict is None:                            predict = output                        else:                            predict = torch.cat([predict, output], dim=1)  # [batch_size, step, node, dim]                        m = m + 1                    cur_modal_mse = F.mse_loss(predict, target_pred)                    if min_loss == 0:                        min_loss = cur_modal_mse                        output_pred = predict                    elif cur_modal_mse < min_loss:                        min_loss = cur_modal_mse                        output_pred = predict            target = states[:, burn_in_index*cfg.re_encoding_gap:, :, :]            # output_all = torch.cat([output_burn[:, 1:, :, :], output_pred], dim=1)            output_all = output_pred            loss_nll = nll_gaussian(output_all, target, cfg.scale)            loss = loss_nll + loss_kl / groups            self.optimize(self.opt, loss * cfg.scale)            minADE.append(scale * min_ADE(denormalize(output_all, 28.65, 15.24), denormalize(target, 28.65, 15.24)))            minFDE.append(scale * min_FDE(denormalize(output_all[:, -1, :, :], 28.65, 15.24), denormalize(target[:, -1, :, :], 28.65, 15.24)))            #minADE.append(scale * min_ADE(output_all, target))            #minFDE.append(scale * min_FDE(output_all[:, -1, :, :], target[:, -1, :, :]))            loss_all += scale * loss        minADE = sum(minADE) / N        minFDE = sum(minFDE) / N        loss_all /= N        self.log.info('trainLoss {:.3e} trainADE {:.4f} trainFDE {:.4f}'.format(loss_all, minADE,                                                                                        minFDE))        return loss_all, minADE, minFDE    """    def old_train_model(self, pattern):        self.model.train()        data = self.load_data(TensorDataset(*self.data['train']), cfg.batch_size)   #[batch_size, steps, node, dim]        loss_a = 0.        N = 0.        minADE, minFDE = [], []        for batch in data:            states, category = batch            states = normalize(states)            if cfg.using_gpu:                states = states.cuda()                category = category.cuda()            scale = len(states) / cfg.batch_size            N += scale            # context_hidden = self.model.context_embedding(context_input)            context_hidden = Variable(torch.zeros(cfg.batch_size, cfg.context_emb_hid))            steps = states.shape[1]            groups = steps // cfg.re_encoding_gap            loss_mse, loss_kl = 0, 0            output_all = None            burn_in_index = self.burn_in_steps / cfg.re_encoding_gap            graph_hidden1, graph_hidden2 = None, None            gru_hidden1, gru_hidden2 = None, None            for m in range(groups):                if m < burn_in_index:                    input = states[:, m * cfg.re_encoding_gap : (m + 1) * cfg.re_encoding_gap, :, :]                    burn_in_flag = True                else:                    input = output_all[:, -cfg.re_encoding_gap:, :, :]                    burn_in_flag = False                edges, prob, graph_hidden1, graph_hidden2 = self.model.module.infer_relations(input, category, context_hidden,                                                                                       graph_hidden1, graph_hidden2,                                                                                       pattern)  # encoding                # edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample                predict = []                temp_gruh1, temp_gruh2 = None, None                for d in range(self.dec_repeat_num):                    temp_gruh1, temp_gruh2 = gru_hidden1, gru_hidden2                    predict_cm, temp_gruh1, temp_gruh2 = self.model.module.predict_states(input, edges, context_hidden, cfg.re_encoding_gap, temp_gruh1,                                                                                   temp_gruh2,                                                                                   burn_in_flag)  # decoding                    predict.append(predict_cm)                output = torch.stack(predict, dim=0)                gru_hidden1, gru_hidden2 = temp_gruh1, temp_gruh2                # target = states[:, (m+1)*cfg.re_encoding_gap:(m+2)*cfg.re_encoding_gap, :, :]                if output_all is None:                    output_all = output                else:                    output_all = torch.cat([output_all, output], dim=2)  # [repeat_num(d), batch_size, step, node, dim]                # loss_nll = nll_gaussian(output, target, cfg.scale)                # loss_mse.append(loss_nll)                # prob = logits.softmax(-1)                loss_kl += entropy(prob, prob) / (prob.shape[1] * states.shape[2])                # cur_loss = loss_nll + loss_kl                # loss_train.append(cur_loss)            target = states[:, 1:, :, :]            min_loss = 0            min_idx = 0            for d in range(self.dec_repeat_num):                cur_modal_mse = F.mse_loss(output_all[d, :, :, :, :], target)                if min_loss == 0:                    min_loss = cur_modal_mse                    min_idx = d                elif cur_modal_mse < min_loss:                    min_loss = cur_modal_mse                    min_idx = d            final_output = output_all[min_idx]            loss_nll = nll_gaussian(final_output, target, cfg.scale)            loss = loss_nll + loss_kl / groups            self.optimize(self.opt, loss * cfg.scale)            minADE.append(scale * min_ADE(denormalize(final_output), denormalize(target)))            minFDE.append(scale * min_FDE(denormalize(final_output[:, -1, :, :]), denormalize(target[:, -1, :, :])))            loss_a += scale * loss        minADE = sum(minADE) / N        minFDE = sum(minFDE) / N        loss_a /= N        return loss_a, minADE, minFDE    """    '''    def train_dynamic(self, states, category, context_hidden):        steps = states.shape[1]        groups = steps // cfg.re_encoding_gap        loss_mse, loss_kl = 0, 0        output_all = None        burn_in_index = self.burn_in_steps / cfg.re_encoding_gap        for m in range(groups):            if m < burn_in_index:                input = states[:, m * cfg.re_encoding_gap: (m + 1) * cfg.re_encoding_gap, :, :]                burn_in_flag = True            else:                input = output_all[:, -cfg.re_encoding_gap:-1, :, :]                burn_in_flag = False            logits = self.dynamic_enc(input, self.size, category, context_hidden)  # encoding            edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample            predict = []            for d in range(self.dec_repeat_num):                predict.append(self.dec(input, self.size, edges, context_hidden, cfg.predict_steps,                                        burn_in_flag=burn_in_flag))  # decoding            output = torch.stack(predict, dim=0)            # target = states[:, (m+1)*cfg.re_encoding_gap:(m+2)*cfg.re_encoding_gap, :, :]            output_all = torch.cat([output_all, output], dim=2)  # [repeat_num(d), batch_size, step, node, dim]            # loss_nll = nll_gaussian(output, target, cfg.scale)            # loss_mse.append(loss_nll)            prob = logits.softmax(-1)            loss_kl += entropy(prob, prob) / (prob.shape[1] * self.size)            # cur_loss = loss_nll + loss_kl            # loss_train.append(cur_loss)        target = states[:, 1:, :, :]        min_loss = 0        min_idx = 0        for d in range(self.dec_repeat_num):            cur_modal_mse = F.mse_loss(output_all[d, :, :, :, :], target)            if min_loss == 0:                min_loss = cur_modal_mse                min_idx = d            elif cur_modal_mse < min_loss:                min_loss = cur_modal_mse                min_idx = d        final_output = output_all[min_idx]        loss_nll = nll_gaussian(final_output, target, cfg.scale)        loss = loss_nll + loss_kl / groups        self.optimize(self.dynamic_opt, loss * cfg.scale)        return loss, modal_idx    '''    def eval(self, pattern):        self.model.eval()        data = self.load_data(TensorDataset(*self.data['val']), cfg.batch_size)        with torch.no_grad():            loss_all = 0.            N = 0.            minADE, minFDE = [], []            for batch in data:                states, category = batch                states = normalize(states)                if cfg.using_gpu:                    states = states.cuda()                    category = category.cuda()                scale = len(states) / cfg.batch_size                N += scale                # context_hidden = self.model.context_embedding(context_input)                batch_size = states.size(0)                context_hidden = Variable(torch.zeros(batch_size, cfg.context_emb_hid))                steps = states.shape[1]                groups = steps // cfg.re_encoding_gap                output_burn = None                burn_in_index = self.burn_in_steps // cfg.re_encoding_gap                loss_mse, loss_kl = 0, 0                graph_hidden1, graph_hidden2 = None, None                gru_hidden1, gru_hidden2 = None, None                group_index = 0                start_states = torch.tensor(states[:, 0, :, :].unsqueeze(1).contiguous(), dtype=torch.float32).cuda()                while group_index < burn_in_index:                    inputs = states[:, group_index * cfg.re_encoding_gap: (group_index + 1) * cfg.re_encoding_gap, :, :]                    burn_in_flag = True                    '''                    edges, prob, graph_hidden1, graph_hidden2 = self.model.module.infer_relations(inputs, category,                                                                                                  context_hidden,                                                                                                  graph_hidden1,                                                                                                  graph_hidden2,                                                                                                  pattern)  # encoding                    # edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample                    # predict = []                    output, gru_hidden1, gru_hidden2 = self.model.module.predict_states(inputs, edges, context_hidden,                                                                                        group_index,                                                                                        gru_hidden1, gru_hidden2,                                                                                        burn_in_flag)                    '''                    edges, prob, graph_hidden1, graph_hidden2, output, gru_hidden1, gru_hidden2 = self.model(inputs,                                                                                                             category,                                                                                                             context_hidden,                                                                                                             graph_hidden1,                                                                                                             graph_hidden2,                                                                                                             pattern,                                                                                                             group_index,                                                                                                             gru_hidden1,                                                                                                             gru_hidden2,                                                                                                             burn_in_flag)                    if output_burn is None:                        output_burn = torch.cat([start_states, output], dim=1)                    else:                        output_burn = torch.cat([output_burn, output], dim=1)  # [batch_size, step, node, dim]                    # loss_nll = nll_gaussian(output, target, cfg.scale)                    # loss_mse.append(loss_nll)                    # prob = logits.softmax(-1)                    loss_kl = entropy(prob, prob) / (prob.shape[1] * states.shape[2])                    # cur_loss = loss_nll + loss_kl                    # loss_train.append(cur_loss)                    group_index = group_index + 1                    """                    target = states[:, 1:burn_in_index*cfg.re_encoding_gap, :, :]                    loss_nll = nll_gaussian(output_burn[:, 1:, :, :], target, cfg.scale)                    loss = loss_nll + loss_kl                    self.optimize(self.opt, loss * cfg.scale)                    """                if group_index < groups:                    target_pred = states[:, burn_in_index * cfg.re_encoding_gap:, :, :]                    min_loss = 0                    output_pred = None                    for d in range(self.dec_repeat_num):                        temp_gruh1, temp_gruh2 = gru_hidden1, gru_hidden2                        temp_graph_hid1, temp_graph_hid2 = graph_hidden1, graph_hidden2                        predict = None                        m = group_index                        while m < groups:                            if predict is None:                                inputs = output_burn[:, -cfg.re_encoding_gap:, :, :]                            else:                                inputs = predict[:, -cfg.re_encoding_gap:, :, :]                            burn_in_flag = False                            '''                            edges, prob, temp_graph_hid1, temp_graph_hid2 = self.model.module.infer_relations(input,                                                                                                              category,                                                                                                              context_hidden,                                                                                                              temp_graph_hid1,                                                                                                              temp_graph_hid2,                                                                                                              pattern)  # encoding                            # edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample                            output, temp_gruh1, temp_gruh2 = self.model.module.predict_states(input, edges,                                                                                              context_hidden, m,                                                                                              temp_gruh1, temp_gruh2,                                                                                              burn_in_flag)                            '''                            edges, prob, temp_graph_hid1, temp_graph_hid2, output, temp_gruh1, temp_gruh2 = self.model(                                inputs, category, context_hidden, temp_graph_hid1, temp_graph_hid2, pattern,                                group_index, temp_gruh1, temp_gruh2, burn_in_flag)                            if predict is None:                                predict = output                            else:                                predict = torch.cat([predict, output], dim=1)  # [batch_size, step, node, dim]                            m = m + 1                        cur_modal_mse = F.mse_loss(predict, target_pred)                        if min_loss == 0:                            min_loss = cur_modal_mse                            output_pred = predict                        elif cur_modal_mse < min_loss:                            min_loss = cur_modal_mse                            output_pred = predict                target = states[:, burn_in_index * cfg.re_encoding_gap:, :, :]                # output_all = torch.cat([output_burn[:, 1:, :, :], output_pred], dim=1)                output_all = output_pred                loss_nll = nll_gaussian(output_all, target, cfg.scale)                loss = loss_nll + loss_kl / groups                #self.optimize(self.opt, loss * cfg.scale)                minADE.append(scale * min_ADE(denormalize(output_all, 28.65, 15.24), denormalize(target, 28.65, 15.24)))                minFDE.append(scale * min_FDE(denormalize(output_all[:, -1, :, :], 28.65, 15.24),                                              denormalize(target[:, -1, :, :], 28.65, 15.24)))                # minADE.append(scale * min_ADE(output_all, target))                # minFDE.append(scale * min_FDE(output_all[:, -1, :, :], target[:, -1, :, :]))                loss_all += scale * loss            minADE = sum(minADE) / N            minFDE = sum(minFDE) / N            loss_all /= N            self.log.info('valLoss {:.3e} valADE {:.4f} valFDE {:.4f}'.format(loss_all, minADE,                                                                                    minFDE))        return minADE, minFDE    def test(self, pattern='dynamic'):        self.model.eval()        data = self.load_data(TensorDataset(*self.data['test']), cfg.batch_size)        with torch.no_grad():            loss_all = 0.            N = 0.            minADE, minFDE = [], []            for batch in data:                states, category = batch                states = normalize(states)                if cfg.using_gpu:                    states = states.cuda()                    category = category.cuda()                scale = len(states) / cfg.batch_size                N += scale                # context_hidden = self.model.context_embedding(context_input)                batch_size = states.size(0)                context_hidden = Variable(torch.zeros(batch_size, cfg.context_emb_hid))                steps = states.shape[1]                groups = steps // cfg.re_encoding_gap                output_burn = None                burn_in_index = self.burn_in_steps // cfg.re_encoding_gap                loss_mse, loss_kl = 0, 0                graph_hidden1, graph_hidden2 = None, None                gru_hidden1, gru_hidden2 = None, None                group_index = 0                start_states = torch.tensor(states[:, 0, :, :].unsqueeze(1).contiguous(), dtype=torch.float32).cuda()                while group_index < burn_in_index:                    inputs = states[:, group_index * cfg.re_encoding_gap: (group_index + 1) * cfg.re_encoding_gap, :, :]                    burn_in_flag = True                    '''                    edges, prob, graph_hidden1, graph_hidden2 = self.model.module.infer_relations(inputs, category,                                                                                                  context_hidden,                                                                                                  graph_hidden1,                                                                                                  graph_hidden2,                                                                                                  pattern)  # encoding                    # edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample                    # predict = []                    output, gru_hidden1, gru_hidden2 = self.model.module.predict_states(inputs, edges, context_hidden,                                                                                        group_index,                                                                                        gru_hidden1, gru_hidden2,                                                                                        burn_in_flag)                    '''                    edges, prob, graph_hidden1, graph_hidden2, output, gru_hidden1, gru_hidden2 = self.model(inputs,                                                                                                             category,                                                                                                             context_hidden,                                                                                                             graph_hidden1,                                                                                                             graph_hidden2,                                                                                                             pattern,                                                                                                             group_index,                                                                                                             gru_hidden1,                                                                                                             gru_hidden2,                                                                                                             burn_in_flag)                    if output_burn is None:                        output_burn = torch.cat([start_states, output], dim=1)                    else:                        output_burn = torch.cat([output_burn, output], dim=1)  # [batch_size, step, node, dim]                    # loss_nll = nll_gaussian(output, target, cfg.scale)                    # loss_mse.append(loss_nll)                    # prob = logits.softmax(-1)                    loss_kl = entropy(prob, prob) / (prob.shape[1] * states.shape[2])                    # cur_loss = loss_nll + loss_kl                    # loss_train.append(cur_loss)                    group_index = group_index + 1                    """                    target = states[:, 1:burn_in_index*cfg.re_encoding_gap, :, :]                    loss_nll = nll_gaussian(output_burn[:, 1:, :, :], target, cfg.scale)                    loss = loss_nll + loss_kl                    self.optimize(self.opt, loss * cfg.scale)                    """                if group_index < groups:                    target_pred = states[:, burn_in_index * cfg.re_encoding_gap:, :, :]                    min_loss = 0                    output_pred = None                    for d in range(self.dec_repeat_num):                        temp_gruh1, temp_gruh2 = gru_hidden1, gru_hidden2                        temp_graph_hid1, temp_graph_hid2 = graph_hidden1, graph_hidden2                        predict = None                        m = group_index                        while m < groups:                            if predict is None:                                inputs = output_burn[:, -cfg.re_encoding_gap:, :, :]                            else:                                inputs = predict[:, -cfg.re_encoding_gap:, :, :]                            burn_in_flag = False                            '''                            edges, prob, temp_graph_hid1, temp_graph_hid2 = self.model.module.infer_relations(input,                                                                                                              category,                                                                                                              context_hidden,                                                                                                              temp_graph_hid1,                                                                                                              temp_graph_hid2,                                                                                                              pattern)  # encoding                            # edges = F.gumbel_softmax(logits, tau=cfg.temp, hard=False)  # sample                            output, temp_gruh1, temp_gruh2 = self.model.module.predict_states(input, edges,                                                                                              context_hidden, m,                                                                                              temp_gruh1, temp_gruh2,                                                                                              burn_in_flag)                            '''                            edges, prob, temp_graph_hid1, temp_graph_hid2, output, temp_gruh1, temp_gruh2 = self.model(                                inputs, category, context_hidden, temp_graph_hid1, temp_graph_hid2, pattern,                                group_index, temp_gruh1, temp_gruh2, burn_in_flag)                            if predict is None:                                predict = output                            else:                                predict = torch.cat([predict, output], dim=1)  # [batch_size, step, node, dim]                            m = m + 1                        cur_modal_mse = F.mse_loss(predict, target_pred)                        if min_loss == 0:                            min_loss = cur_modal_mse                            output_pred = predict                        elif cur_modal_mse < min_loss:                            min_loss = cur_modal_mse                            output_pred = predict                target = states[:, burn_in_index * cfg.re_encoding_gap:, :, :]                # output_all = torch.cat([output_burn[:, 1:, :, :], output_pred], dim=1)                output_all = output_pred                loss_nll = nll_gaussian(output_all, target, cfg.scale)                loss = loss_nll + loss_kl / groups                #self.optimize(self.opt, loss * cfg.scale)                minADE.append(scale * min_ADE(denormalize(output_all, 28.65, 15.24), denormalize(target, 28.65, 15.24)))                minFDE.append(scale * min_FDE(denormalize(output_all[:, -1, :, :], 28.65, 15.24),                                              denormalize(target[:, -1, :, :], 28.65, 15.24)))                # minADE.append(scale * min_ADE(output_all, target))                # minFDE.append(scale * min_FDE(output_all[:, -1, :, :], target[:, -1, :, :]))                loss_all += scale * loss            minADE = sum(minADE) / N            minFDE = sum(minFDE) / N            loss_all /= N            self.log.info('testLoss {:.3e} testADE {:.4f} testFDE {:.4f}'.format(loss_all, minADE,                                                                              minFDE))        return minADE, minFDE
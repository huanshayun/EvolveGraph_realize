from model.base import MLP, CirConv1dfrom model.gnn import GNN, MyGATfrom utils.general import prodfrom utils.torch_extension import my_bnfrom torch import nnfrom torch.autograd import Variablefrom torch_geometric.utils import softmax as gsoftmaxfrom itertools import chain, combinations, permutationsimport torchimport torch.nn.functional as F#from torch.nn.modules import BatchNorm1dimport mathimport numpy as npimport config as cfgimport timeclass GRUX(nn.Module):    def __init__(self, dim_in, dim_hid, bias=True):        super(GRUX, self).__init__()        self.hidden = nn.ModuleList([            nn.Linear(dim_hid, dim_hid, bias)            for _ in range(3)        ])        self.input = nn.ModuleList([            nn.Linear(dim_in, dim_hid, bias)            for _ in range(3)        ])    def forward(self, inputs, hidden, state=None):        r = torch.sigmoid(self.input[0](inputs) + self.hidden[0](hidden))        z = torch.sigmoid(self.input[1](inputs) + self.hidden[1](hidden))        n = torch.tanh(self.input[2](inputs) + r * self.hidden[2](hidden))        if state is None:            state = hidden        output = (1 - z) * n + z * state        return outputclass TripleMLP(nn.Module):    """Three-layer fully-connected ELU net with batch norm."""    def __init__(self, n_in, n_hid, n_out, do_prob=0.):        super(TripleMLP, self).__init__()        self.fc1 = nn.Linear(n_in, n_hid)        self.fc2 = nn.Linear(n_hid, n_hid)        self.fc3 = nn.Linear(n_hid, n_out)        self.bn1 = nn.BatchNorm1d(n_hid)        # added        self.bn2 = nn.BatchNorm1d(n_hid)        #self.bn3 = nn.BatchNorm1d(n_out)        self.dropout_prob = do_prob        # self.init_weights()    def init_weights(self):        for m in self.modules():            if isinstance(m, nn.Linear):                nn.init.xavier_normal_(m.weight.data)                m.bias.data.fill_(0.1)            elif isinstance(m, nn.BatchNorm1d):                m.weight.data.fill_(1)                m.bias.data.zero_()    def batch_norm(self, inputs, bn):        x = inputs.view(prod(inputs.shape[:-1]), -1)        x = bn(x)        return x.view(*inputs.shape[:-1], -1)    def forward(self, inputs):        # Input shape: [num_sims, num_things, num_features]        x = F.elu(self.fc1(inputs))        # added        x = self.batch_norm(x, self.bn1)        x = F.dropout(x, self.dropout_prob, training=self.training)        x = F.elu(self.fc2(x))        # added        x = self.batch_norm(x, self.bn2)        x = F.dropout(x, self.dropout_prob, training=self.training)        x = self.fc3(x)        # added        #x = self.batch_norm(x, self.bn3)        #x = F.dropout(x, self.dropout_prob, training=self.training)        return xclass static_graph_learning(GNN):    def __init__(self, agent_in, context_hid, edge_out, do_prob = 0.):        super(static_graph_learning, self).__init__()        self.agent_in = agent_in        #self.context_in = context_in        self.agent_hid = cfg.agent_emb_hid        self.context_hid = context_hid        self.edge_hid = cfg.edge_update_hid        self.edge_out = cfg.edge_types        self.gru_in = nn.Sequential(nn.Linear(self.agent_in, self.agent_hid), nn.ReLU(), nn.Dropout(do_prob))        self.agent_emb1 = nn.ModuleList([            GRUX(self.agent_hid, self.agent_hid)            for _ in range(cfg.agent_cat)        ])        self.agent_emb2 = nn.ModuleList([            GRUX(self.agent_hid, self.agent_hid)            for _ in range(cfg.agent_cat)        ])        self.gru_out = nn.Sequential(nn.Linear(self.agent_hid, self.agent_hid), nn.ReLU(), nn.Dropout(do_prob))        self.agent2agent_edge_update = TripleMLP(2 * self.agent_hid, self.edge_hid, self.edge_hid)        self.context2agent_edge_update = TripleMLP(self.context_hid + self.agent_hid, self.edge_hid, self.edge_hid)        self.agent_node_update = TripleMLP(2 * self.edge_hid, self.agent_hid, self.agent_hid)        self.edge_out = TripleMLP(4 * self.agent_hid, self.edge_hid, self.edge_out)        self.query = nn.Sequential(nn.Linear(self.agent_hid, cfg.enc_att_hid), nn.ReLU(), nn.Dropout(do_prob))        self.key = nn.Sequential(nn.Linear(self.agent_hid, cfg.enc_att_hid), nn.ReLU(), nn.Dropout(do_prob))        self.value = nn.Sequential(nn.Linear(self.agent_hid, cfg.enc_att_hid), nn.ReLU(), nn.Dropout(do_prob))        self.att_out = nn.Sequential(nn.Linear(cfg.enc_att_hid, self.agent_hid), nn.ReLU(), nn.Dropout(do_prob))        self.att_lin = nn.Linear(self.agent_hid * 2, 1)    def init_weights(self):        for m in self.modules():            if isinstance(m, nn.Linear):                nn.init.xavier_normal_(m.weight.data)                m.bias.data.fill_(0.1)    def agent_embedding(self, x, category):        """        :param x: [node, batch, step, dim]        :param category: [batch, node]        :return: agent_self_hid: [node, batch, dim(agent_hid)]        """        classes = cfg.agent_cat        # shape: [node, batch, step, dim]        shape = x.shape        #x = x.view(shape[0] * shape[1], shape[2], shape[-1])        gru_hidden1, gru_hidden2 = None, None        agent_hidden = Variable(torch.zeros([shape[0], shape[1], self.agent_hid]))        if x.is_cuda:            agent_hidden = agent_hidden.cuda(x.device)        category = torch.transpose(category, 0, 1) #[batch, node]        '''        ## get idx        batch, node, step, dim = 16, 10, 50, 2        classes = 5        hidden = 256        # node class        x = torch.randint(0, classes, (batch, node))        # node state        y = torch.rand(batch, node, step, dim)        # class specific mlps        mlps = torch.nn.ModuleList([torch.nn.Linear(dim, hidden) for _ in range(classes)])        # index        idx = [x == i for i in range(classes)]  # for each category, determine whether an element in X belongs to that category        hs = [mlps[i](y[j]) for i, j in zip(range(classes), idx)]        h = torch.zeros(batch, node, step, hidden)         for i, j in zip(range(classes), idx):            h[j] = hs[i]        pass        '''        idx = [category == i for i in range(classes)]        for m, index in zip(range(classes), idx):            x_select = x[index]            for step in range(shape[2]):                if gru_hidden1 is None:                    gru_hidden1 = x_select[:, step, :]                    gru_hidden2 = x_select[:, step, :]                else:                    gru_hidden1 = self.agent_emb1[m](x_select[:, step, :], gru_hidden1)                    gru_hidden2 = self.agent_emb2[m](gru_hidden1, gru_hidden2)            # gru_hidden2: [this_cat_num, agent_hid]            #x_cat_emb = gru_hidden2.squeeze(1).contiguous()            agent_hidden[index] = gru_hidden2            gru_hidden1, gru_hidden2 = None, None        #for m, index in zip(range(classes), idx):            #agent_hidden[index] = x_hidden[m]        #agent_hidden.view(shape[0], shape[1], shape[2], shape[-1])        return agent_hidden    """    def spatial_attention(self, x):        # x: [node, batch, dim(agent_hid)]        x = x.permute(1, 0, -1).contiguous()  # x: [batch, node, dim(agent_hid)]        query = self.query(x)   # query: [batch, node, dim(att_hid)]        key = self.key(x)        value = self.value(x)        key = key.permute(0, -1, 1).contiguous() # key: [batch, dim(att_hid), node]        attention = torch.matmul(query, key)    #att: [batch, node, node]        attention /= (cfg.enc_att_hid ** 0.5)        # mask        size = x.shape[1]        mask = torch.eye(size).cuda()        mask = mask.repeat(x.shape[0], 1, 1).contiguous()        mask = mask > 0        minValue = torch.Tensor([-2 ** 15 + 1]).cuda().expand(attention.size())        attention = torch.where(mask, minValue, attention)        attention = F.softmax(attention, dim=-1)        #att_value = torch.matmul(attention, value) # att_value: [batch, node, dim(att_hid)]        #att_out = self.att_out(att_value)  # att_out: [batch, node, dim(agent_hid)]        return attention    def edge_att(self, z, size, attention, es):        # z : [E, batch, dim(edge_hid)], E=[[1,0],[2,0],[3,0],[4,0], [0,1],[2,1],[3,1],[4,1], ..., [0,4],[1,4],[2,4],[3,4]]        # es: [2, E],        # att: [batch, node, node]        col, row = es        idx = 0        att_hid = z.permute(1, 0, -1).contiguous() #[batch, E, dim(edge_hid)]        z_att_hid = z.permute(1, 0, -1).contiguous()        #attention = attention.permute(1, 2, 0).contiguous() #[node, node, batch]        for r, c in zip(row, col):            z_att_hid[:, idx, :] = torch.mul(att_hid[:, idx, :], attention[:, c, r].unsqueeze(0).repeat(z.shape[-1], 1).contiguous().T)            idx = idx+1        z_att_hid = z_att_hid.permute(1, 0, -1).contiguous()        return z_att_hid    """    def edge_attention(self, x, es):        msg, col, size = self.message(x, es) #msg: [E, batch, dim(agent_hid)]        alpha = self.att_lin(msg).squeeze(-1) #[E, batch]        att = gsoftmax(alpha, col, size) #[E, batch]        spatial_attention = att.unsqueeze(-1).repeat(1, 1, self.edge_hid).contiguous()        return spatial_attention    def forward(self, inputs, category, context_hidden):        """        input(which mean agent-node) : [batch_size, step(time_window), node_num, dim]        category : [batch_size, node, 1(category-feature)]        context_hidden : [batch_size, dim(context_hid)]        """        size = inputs.shape[2]        es = np.array(list(permutations(range(size), 2))).T        es = torch.LongTensor(es)        if inputs.is_cuda:            es = es.cuda(inputs.device)            context_hidden = context_hidden.cuda(inputs.device)        # x:[node, batch, step, dim]        x = inputs.permute(2, 0, 1, -1).contiguous()        #agent_self:[node, batch, dim(agent_hid)]        x = self.gru_in(x)        agent_self_hid = self.agent_embedding(x, category)        """ node2edge """        z_agent, col, size = self.message(agent_self_hid, es) #[E, batch, dim(agent_hid * 2)]        agent2agent_edge_hidden = self.agent2agent_edge_update(z_agent)  # [E, batch, dim(edge_hid)]        """ context node message """        context_hidden = context_hidden.unsqueeze(0).contiguous()        context_hidden = context_hidden.repeat(size, 1, 1)  #context_hidden : [node, batch_size, dim(context_hid)]        z_context = torch.cat([agent_self_hid, context_hidden], dim=-1)  #[node, batch_size, dim(agent_hid + context_hid)]        context2agent_edge_hidden = self.context2agent_edge_update(z_context) #[node, batch_size, dim(edge_hid)]        """ spatial attention and edge2node """        # spatio_att = self.spatial_attention(agent_self_hid) # [E, batch, dim(att_value = 1)]        # edge_att_hid = self.edge_att(agent2agent_edge_hidden, size, spatio_att, es) # [E, batch, dim(edge_hid)]        edge_att_hid = agent2agent_edge_hidden * self.edge_attention(agent_self_hid, es) # [E, batch, dim(edge_hid)]        agent_aggregate = self.aggregate(edge_att_hid, col, size) #[node, batch_size, dim(edge_hid)]        agent_social_hid = self.agent_node_update(torch.cat([agent_aggregate, context2agent_edge_hidden], dim=-1)) #[node, batch_size, dim(agent_hid)]        """ node2edge """        node_out = torch.cat([agent_self_hid, agent_social_hid], dim=-1) #[node, batch_size, dim(agent_hid * 2)]        edge_final, _, _ = self.message(node_out, es) # [E, batch, dim(agent_hid * 4)]        graph_out = self.edge_out(edge_final)  # [E, batch, dim(edge_out = edge_type)]        return graph_outclass dynamic_graph_learning(GNN):    def __init__(self, agent_in, context_hid, edge_out, do_prob=0.):        super(dynamic_graph_learning, self).__init__()        self.reencoding_gap = cfg.re_encoding_gap        self.gru_in = nn.Sequential(nn.Linear(edge_out, cfg.graph_gru_hid), nn.ReLU(), nn.Dropout(do_prob))        self.graph_gru1 = GRUX(cfg.graph_gru_hid, cfg.graph_gru_hid)        self.graph_gru2 = GRUX(cfg.graph_gru_hid, cfg.graph_gru_hid)        self.gru_out = nn.Linear(cfg.graph_gru_hid, edge_out)        #self.static_step = static_step        self.static_enc = static_graph_learning(agent_in, context_hid, edge_out)    def init_weights(self):        for m in self.modules():            if isinstance(m, nn.Linear):                nn.init.xavier_normal_(m.weight.data)                m.bias.data.fill_(0.1)    def forward(self, states, category, context_hidden, graph_hidden1=None, graph_hidden2=None, pattern='static'):        # print("in_s=", inputs.shape)        # inputs: [batch, step(time_window), node, dim]        batch, step, node, dim = states.shape        #group =        # step // self.reencoding_gap        inputs = torch.tensor(states, dtype=torch.float32).cuda(states.device)        static_graph = self.static_enc(inputs, category, context_hidden)        assert pattern in {'static', 'dynamic'}        if pattern == 'static':            return static_graph, graph_hidden1, graph_hidden2        else:            if graph_hidden1 is not None:                graph_hidden1 = self.graph_gru1(self.gru_in(static_graph), graph_hidden1)                graph_hidden2 = self.graph_gru2(graph_hidden1, graph_hidden2)                dynamic_graph = self.gru_out(graph_hidden2)            else:                graph_hidden1 = self.gru_in(static_graph)                graph_hidden2 = self.gru_in(static_graph)                dynamic_graph = static_graph            return dynamic_graph, graph_hidden1, graph_hidden2